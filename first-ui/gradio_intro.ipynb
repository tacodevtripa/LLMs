{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b0e11f2-9ea4-48c2-b8d2-d0a4ba967827",
   "metadata": {},
   "source": [
    "# Introduccion a Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c44c5494-950d-4d2f-8d4f-b87b57c5b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from typing import List\n",
    "from langchain_ollama.llms import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1715421-cead-400b-99af-986388a97aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b16e6021-6dc4-4397-985a-6679d6c8ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02ef9b69-ef31-427d-86d0-b8c799e1c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_ollama(prompt, model_name):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    return OllamaLLM(model=model_name).invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7d314-2b13-436b-b02d-8de3b72b193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is Gradio?\"\n",
    "qwen_05b = \"qwen2.5-coder:0.5b\"\n",
    "message_ollama(prompt, qwen_05b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94013d1-4f27-4329-97e8-8c58db93636a",
   "metadata": {},
   "source": [
    "## Ejemplo de User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc664b7a-c01d-4fea-a1de-ae22cdd5141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shout(text):\n",
    "    print(f\"Shout has been called with input {text}\")\n",
    "    return text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083ea451-d3a0-4d13-b599-93ed49b975e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shout(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f1f15a-122e-4502-b112-6ee2817dda32",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a359a4-685c-4c99-891c-bb4d1cb7f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir share=True significa que puede ser accedido públicamente\n",
    "# NOTA: Algunos antivírus y firewall corporativos pueden no tolerar usar share=True. Si estás en un entorno de trabajo o en una red de trabajo, sugiero desactivar esta prueba.\n",
    "\n",
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd87533a-ff3a-4188-8998-5bedd5ba2da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar inbrowser=True abre una ventana de navegador automaticamente\n",
    "\n",
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42ec007-0314-48bf-84a4-a65943649215",
   "metadata": {},
   "source": [
    "## Forzar 'Dark Mode'\n",
    "\n",
    "Gradio se muestra en modo claro o oscuro dependiendo del conjunto de configuraciones del navegador y la\n",
    "computadora. Hay una forma de forzar Gradio a mostrar el modo oscuro, pero Gradio recomienda no hacer esto ya que\n",
    "debería ser una preferencia de accesibilidad ( especialmente para los usuarios). Sin embargo, para hacerlo siga estos pasos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8129afa-532b-4b15-b93c-aa9cca23a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define esta variable y luego pasa \"force_dark_mode\" al crear la interfaz.\n",
    "\n",
    "force_dark_mode = \"\"\"\n",
    "function refresh() {\n",
    "    const url = new URL(window.location);\n",
    "    if (url.searchParams.get('__theme') !== 'dark') {\n",
    "        url.searchParams.set('__theme', 'dark');\n",
    "        window.location.href = url.href;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\", js=force_dark_mode).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc67b26-dd5f-406d-88f6-2306ee2950c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs y Outputs\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=shout,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\", lines=6)],\n",
    "    outputs=[gr.Textbox(label=\"Response:\", lines=8)],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f235288e-63a2-4341-935b-1441f9be969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar la funcion de demo por la que hace el llamado a nuestro LLM\n",
    "model_value = gr.Text(label=\"LLM Model\", value=qwen_05b)\n",
    "view = gr.Interface(\n",
    "    fn=message_ollama,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\", lines=6), model_value],\n",
    "    outputs=[gr.Textbox(label=\"Response:\", lines=8)],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a3262-e626-4e4b-80b0-aca152405e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiemos un poco el estilo de respuesta para obtenerlo como Markdown\n",
    "\n",
    "system_message = \"You are a helpful assistant that responds in markdown\"\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=message_ollama,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\"), model_value],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88c04ebf-0671-4fea-95c9-bc1565d4bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora modifiquemos el llamado al LLM para que devuela la respuesta en modo de 'stream'\n",
    "def stream_ollama(prompt, model_name):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    result = \"\"\n",
    "    for chunk in OllamaLLM(model=model_name).stream(messages):\n",
    "        result += chunk or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb1f789-ff11-4cba-ac67-11b815e29d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_ollama,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\"), model_value],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0087623a-4e31-470b-b2e6-d8d16fc7bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_multi_model(prompt, model):\n",
    "    if model==\"qwen2.5-coder:0.5b\":\n",
    "        result = stream_ollama(prompt, \"qwen2.5-coder:0.5b\")\n",
    "    elif model==\"llama3.2\":\n",
    "        result = stream_ollama(prompt, \"llama3.2\")\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ce810-997c-4b6a-bc4f-1fc847ac8855",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_multi_model,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\"), gr.Dropdown([\"qwen2.5-coder:0.5b\", \"llama3.2\"], label=\"Select model\", value=\"qwen2.5-coder:0.5b\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d933865b-654c-4b92-aa45-cf389f1eda3d",
   "metadata": {},
   "source": [
    "# Building a company brochure generator\n",
    "\n",
    "Now you know how - it's simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1626eb2e-eee8-4183-bda5-1591b58ae3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        self.body = response.content\n",
    "        soup = BeautifulSoup(self.body, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"Company Name\"\n",
    "        if soup.body:\n",
    "            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                irrelevant.decompose()\n",
    "            self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            self.text = \"\"\n",
    "        # Creación de una lista que contiene todos los links, utilizando find_all para obtener todas las equitetas<a href=\"\"></a> de html\n",
    "        links = [link.get('href') for link in soup.find_all('a')]\n",
    "        # asignando dicha lista al atributo links de cada instancia de clase, solo incluyendo aquellos que empiecen con `/` o `https`\n",
    "        links = [link for link in links if link and (link.startswith('/') or link.startswith('https'))]\n",
    "        for i, elem in enumerate(links):\n",
    "            if elem.startswith('/'):\n",
    "                links[i] = url + elem\n",
    "        # print(f\"de {len(links)} enlaces\")\n",
    "        # Filtrado de links relevantes\n",
    "        # Definimos las palabras de interes\n",
    "        common_keywords = ['company', 'about', 'contact', 'support', 'team', 'careers']\n",
    "        # Calcular un puntaje de relevancia basado en el número de palabras clave comunes encontradas en la ruta del URL\"\n",
    "        scores = {link: sum(1 for keyword in common_keywords if re.search(r'\\b' + keyword + r'\\b', urlparse(link).path)) for link in links}\n",
    "        # Ordenar los enlaces según su puntaje de relevancia y filtrar los mas relevantes hasta alcanzar un determinado número de enlaces\n",
    "        num_relevant_links = int(len(links) * 0.2)  # Porcentaje de enlaces totales a mantener = 20%\n",
    "        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        filtered_links = [link for link, score in sorted_scores[:num_relevant_links]]\n",
    "        # print(f\"a {len(filtered_links)} enlaces\")\n",
    "        self.links = filtered_links\n",
    "\n",
    "\n",
    "    def get_contents(self):\n",
    "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\"\n",
    "\n",
    "    def get_all_details(self):\n",
    "        result = \"Landing page:\\n\"\n",
    "        result += self.get_contents()\n",
    "        for link in self.links:\n",
    "            result += f\"{link}\\n\"\n",
    "            result += Website(link).get_contents()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e24707",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain = Website('https://www.langchain.com')\n",
    "print(langchain.get_all_details())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c701ec17-ecd5-4000-9f68-34634c8ed49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyzes the contents of several relevant pages from a company website \\\n",
    "and creates a short brochure about the company for prospective customers, investors and recruits. Respond in markdown.\\\n",
    "Include details of company culture, customers and careers/jobs if you have the information.\"\n",
    "\n",
    "def get_brochure_user_prompt(website):\n",
    "    user_prompt = f\"You are looking at a company called: {website.title}\\n\"\n",
    "    user_prompt += f\"Here are the contents of its landing page and other relevant pages; use this information to build a short brochure of the company in markdown.\\n\"\n",
    "    user_prompt += website.get_all_details()\n",
    "    user_prompt = user_prompt[:5_000] # Truncate if more than 5,000 characters\n",
    "    return user_prompt\n",
    "\n",
    "def messages_for_LLM(system_prompt, website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt}, #configuracion del system prompt\n",
    "        {\"role\": \"user\", \"content\": get_brochure_user_prompt(website)} #configuracion del input de usuario con los datos de la pagina web\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5def90e0-4343-4f58-9d4a-0e36e445efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nueva implementacion para tener los prompts definidos en el bloque anterior\n",
    "def stream_ollama_for_multi_model(prompt, model_name):\n",
    "    result = \"\"\n",
    "    for chunk in OllamaLLM(model=model_name).stream(prompt):\n",
    "        result += chunk or \"\"\n",
    "        yield result\n",
    "\n",
    "def stream_multi_model(url, model):\n",
    "    website = Website(url)\n",
    "    prompt = messages_for_LLM(system_prompt, website)\n",
    "    if model==\"qwen2.5-coder:0.5b\":\n",
    "        result = stream_ollama_for_multi_model(prompt, \"qwen2.5-coder:0.5b\")\n",
    "    elif model==\"llama3.2\":\n",
    "        result = stream_ollama_for_multi_model(prompt, \"llama3.2\")\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66399365-5d67-4984-9d47-93ed26c0bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_multi_model,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Landing page URL including http:// or https://\"),\n",
    "        gr.Dropdown([\"qwen2.5-coder:0.5b\", \"llama3.2\"], label=\"Select model\")],\n",
    "    outputs=[gr.Markdown(label=\"Brochure:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms-tacodevtripa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
