{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2",
   "metadata": {},
   "source": [
    "# Nuestro primer Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6541d58e-2297-4de1-b1f7-77da1b98b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos modelo de LLM\n",
    "MODEL = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d581f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamada de prueba\n",
    "OllamaLLM(model=MODEL).invoke(\"hello there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e16839b5-c03b-4d9d-add6-87a0f6f37575",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eacc8a4-4b48-4358-9e06-ce0020041bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    print(\"Historial actual:\")\n",
    "    print(history)\n",
    "    print(\"Mensaje por procesar:\")\n",
    "    print(messages)\n",
    "\n",
    "    # misma plantilla de siempre para obtener respuesta en forma de Stream\n",
    "    result = \"\"\n",
    "    for chunk in OllamaLLM(model=MODEL).stream(messages):\n",
    "        result += chunk or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2d5ad-e907-465e-8397-3120583a5bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# el siguiente metodo para la interfaz funciona a partir de gradio 5.12, verifiquemos la version con este comando\n",
    "!pip show gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334422a-808f-4147-9c4c-57d63d9780d0",
   "metadata": {},
   "source": [
    "## Gracias a Gradio, tan solo necesitamos una linea para tener todo configurado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866ca56-100a-44ab-8bd0-1568feaf6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f91b414-8bab-472d-b9c9-3fa51259bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant in a technology store. You should try to gently encourage \\\n",
    "the customer to try items that are on sale. Laptops are 20% off, and most other items are 50% off. \\\n",
    "For example, if the customer says 'I'm looking to buy a computer', \\\n",
    "you could reply something like, 'Wonderful - we have lots options - including several that are part of our sales evemt.'\\\n",
    "Encourage the customer to buy laptos if they are unsure what to get.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e5be3ec-c26c-42bc-ac16-c39d369883f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    result = \"\"\n",
    "    for chunk in OllamaLLM(model=MODEL).stream(messages):\n",
    "        result += chunk or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e9e4e-7836-43ac-a0c3-e1ab5ed6b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d75f0ffa-55c8-4152-b451-945021676837",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt += \"\\nIf the customer asks for graphic card, you should respond that graphic cards are not on sale today, \\\n",
    "but remind the customer to look at laptops!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c602a8dd-2df7-4eb7-b539-4e01865a6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a987a66-1061-46d6-a83a-a30859dc88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "\n",
    "    relevant_system_message = system_prompt\n",
    "    # implementacion basica para buscar agregar cierto contexto a partir de cierta condicion, solo es un ejemplo, para nada recomendado para ambiente de produccion\n",
    "    if 'graphic card' or 'gpu' in message:\n",
    "        relevant_system_message += \" The graphic cards are not on sale today,s; if you are asked for them, be sure to point out other items on sale.\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": relevant_system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    result = \"\"\n",
    "    for chunk in OllamaLLM(model=MODEL).stream(messages):\n",
    "        result += chunk or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20570de2-eaad-42cc-a92c-c779d71b48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms-tacodevtripa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
